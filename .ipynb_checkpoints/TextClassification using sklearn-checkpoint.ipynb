{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19997\n",
      "['newsgroups\\\\alt.atheism', 'newsgroups\\\\comp.graphics', 'newsgroups\\\\comp.os.ms-windows.misc', 'newsgroups\\\\comp.sys.ibm.pc.hardware', 'newsgroups\\\\comp.sys.mac.hardware', 'newsgroups\\\\comp.windows.x', 'newsgroups\\\\misc.forsale', 'newsgroups\\\\rec.autos', 'newsgroups\\\\rec.motorcycles', 'newsgroups\\\\rec.sport.baseball', 'newsgroups\\\\rec.sport.hockey', 'newsgroups\\\\sci.crypt', 'newsgroups\\\\sci.electronics', 'newsgroups\\\\sci.med', 'newsgroups\\\\sci.space', 'newsgroups\\\\soc.religion.christian', 'newsgroups\\\\talk.politics.guns', 'newsgroups\\\\talk.politics.mideast', 'newsgroups\\\\talk.politics.misc', 'newsgroups\\\\talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "#Traversing the path and enlisting the folders and files\n",
    "x = [os.path.join(r,file) for r,d,f in os.walk(r'''newsgroups''') for file in f]\n",
    "print(len(x))\n",
    "y = [os.path.join(r,direc) for r,d,f in os.walk(r'''newsgroups''') for direc in d]\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19997\n"
     ]
    }
   ],
   "source": [
    "#Spliting the files into Train and Test files\n",
    "test_file = []\n",
    "train_file = x\n",
    "print(len(train_file))\n",
    "for i in range(0,19996,50):\n",
    "    test_file.append(x[i])\n",
    "for file in test_file:\n",
    "     train_file.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "19597\n"
     ]
    }
   ],
   "source": [
    "print(len(test_file))\n",
    "print(len(train_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a complete list of stop words which also includes some special characters present in the texts of data\n",
    "from stop_words import get_stop_words\n",
    "stop_words = get_stop_words('english')\n",
    "del stop_words[-1]\n",
    "stop_words_i = [word.title() for word in stop_words]\n",
    "stop_words_u = [word.upper() for word in stop_words]\n",
    "others = [str(i) for i in range(101)] + [str(i)+\".\" for i in range(101)]\n",
    "sp = [\"''\",'\"\"',\"'\",'\"','!','~','`','!','@','#','$','%','^','&','*','/','-','+','_',':)','**','***','_/','----------------------------------------------------------------------','[]','{}','()','\\\\','|','[',']','(','|>','>>','<<',\n",
    "')','{','}',',','.','?','=','<=','>=','<','>',':',';','A','B','Q','W','E','R','T','Y','U','I','O','P','S','D','F','G','H','J','K','L','Z','X','C','V','N','M','A.','B.','Q.','W.','E.','R.','T.','Y.','U.','I.','O.','P.','S.','D.','F.','G.','H.','J.','K.','L.','Z.','X.','C.','V.','N.','M.']\n",
    "stop_words = stop_words + stop_words_i + stop_words_u + others + sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the vocabulary \n",
    "vocab = {}\n",
    "for j in range(0,len(train_file)):\n",
    "    text = word_tokenize(open(train_file[j]).read()) #Converting string words into tokens\n",
    "    for k in text:\n",
    "        if k not in stop_words:\n",
    "            if vocab.get(k)!= None:\n",
    "                vocab[k]+=1\n",
    "            else:\n",
    "                vocab[k]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = vocab\n",
    "voc = sorted(voc.items(), key=lambda x: x[1], reverse=True) #Sorting the words according to their frequencies in the dataset\n",
    "col = voc[0:2000] #Choosing top 2000 most occuring words\n",
    "col = dict(col) \n",
    "columns = list(col.keys()) #list of all 2000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating 5 2-D arrays with every value 0\n",
    "# getting the memory error if done in one single matrix thats why created 5\n",
    "a1 = np.array([[0] * len(col) for _ in range(5000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2 = np.array([[0] * len(col) for _ in range(5000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a3 = np.array([[0] * len(col) for _ in range(5000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a4 = np.array([[0] * len(col) for _ in range(4000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 2000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traversing the text files once again \n",
    "# Travering each word and marking its frequency if it one of the most occuring word\n",
    "# Doing same for all 5 2D arrays\n",
    "# Converting the arrays into dataframes\n",
    "\n",
    "for j in range(0,5000):\n",
    "    text = word_tokenize(open(train_file[j]).read())\n",
    "    for k in text:\n",
    "        if k in columns:\n",
    "            pos = columns.index(k)\n",
    "            a1[j][pos] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1= pd.DataFrame(data= a1, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0,5000):\n",
    "    text = word_tokenize(open(train_file[j+5000]).read())\n",
    "    for k in text:\n",
    "        if k in columns:\n",
    "            pos = columns.index(k)\n",
    "            a2[j][pos] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2= pd.DataFrame(data= a2, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0,5000):\n",
    "    text = word_tokenize(open(train_file[j+10000]).read())\n",
    "    for k in text:\n",
    "        if k in columns:\n",
    "            pos = columns.index(k)\n",
    "            a3[j][pos] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3= pd.DataFrame(data= a3, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0,4000):\n",
    "    text = word_tokenize(open(train_file[j+15000]).read())\n",
    "    for k in text:\n",
    "        if k in columns:\n",
    "            pos = columns.index(k)\n",
    "            a4[j][pos] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "d4= pd.DataFrame(data= a4, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a5 = np.array([[0] * len(col) for _ in range(597)])\n",
    "\n",
    "for j in range(len(a5)):\n",
    "    text = word_tokenize(open(train_file[j+19000]).read())\n",
    "    for k in text:\n",
    "        if k in columns:\n",
    "            pos = columns.index(k)\n",
    "            a5[j][pos] += 1\n",
    "\n",
    "d5= pd.DataFrame(data= a5, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending all 5 dataframes into 1\n",
    "df = d1.append([d2,d3,d4,d5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using dataframe as matrix once again\n",
    "data = df.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving dataframe into a CSV file\n",
    "df.to_csv(\"train_text.csv\", sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19597, 2000)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a names of the groups files belong to\n",
    "t = []\n",
    "for group in train_file:\n",
    "    g = group.split('\\\\')[1]\n",
    "    t.append(g)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Making a dictionary of groups to give them an integer value which shows class\n",
    "#  I have classified 20 groups into 6 classes viz, Religion, Computer, Vehicles, Sports, Politics and for_sale\n",
    "dic = {'alt.atheism':1,\n",
    " 'comp.graphics':2,\n",
    " 'comp.os.ms-windows.misc':2,\n",
    " 'comp.sys.ibm.pc.hardware':2,\n",
    " 'comp.sys.mac.hardware':2,\n",
    " 'comp.windows.x':2,\n",
    " 'misc.forsale':3,\n",
    " 'rec.autos':4,\n",
    " 'rec.motorcycles':4,\n",
    " 'rec.sport.baseball':4,\n",
    " 'rec.sport.hockey':4,\n",
    " 'sci.crypt':5,\n",
    " 'sci.electronics':5,\n",
    " 'sci.med':5,\n",
    " 'sci.space':5,\n",
    " 'soc.religion.christian':1,\n",
    " 'talk.politics.guns':6,\n",
    " 'talk.politics.mideast':6,\n",
    " 'talk.politics.misc':6,\n",
    " 'talk.religion.misc':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maping from the dictionary the integer value to each group\n",
    "target = []\n",
    "for group in train_file:\n",
    "    g = group.split('\\\\')[1]\n",
    "    target.append(dic[g])\n",
    "\n",
    "# Now we have 2 datasets one data which contains the frequencies of words\n",
    "# second is target datasets containg the classes of texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4, 5, 6}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spliting the Train data \n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(data, target, random_state = 1, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(Xtrain,Ytrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82838589981447119"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(Xtest,Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doing same procedure for Test_file also \n",
    "a_test = np.array([[0] * len(col) for _ in range(len(test_file))])\n",
    "\n",
    "for j in range(len(a_test)):\n",
    "    text = word_tokenize(open(test_file[j]).read())\n",
    "    for k in text:\n",
    "        if k in columns:\n",
    "            pos = columns.index(k)\n",
    "            a_test[j][pos] += 1\n",
    "            \n",
    "d_test= pd.DataFrame(a_test, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test.to_csv(\"test_text.csv\", sep = ',')\n",
    "data_test = d_test.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_test = []\n",
    "for group in test_file:\n",
    "    g = group.split('\\\\')[1]\n",
    "    target_test.append(dic[g])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79500000000000004"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(data_test, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytrain_predict = clf.predict(Xtest)\n",
    "Ytest_predict = clf.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.94      0.86      0.90       942\n",
      "          2       0.99      0.62      0.76      1650\n",
      "          3       0.35      0.94      0.51       326\n",
      "          4       0.89      0.95      0.92      1284\n",
      "          5       0.82      0.88      0.85      1284\n",
      "          6       0.90      0.88      0.89       982\n",
      "\n",
      "avg / total       0.88      0.83      0.84      6468\n",
      "\n",
      "[[ 813    0   13   26   10   80]\n",
      " [   4 1027  379   47  193    0]\n",
      " [   1    0  305   11    9    0]\n",
      " [   1    0   45 1221   13    4]\n",
      " [  10    9   90   40 1128    7]\n",
      " [  35    1   34   24   24  864]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(Ytest,Ytrain_predict))\n",
    "print(confusion_matrix(Ytest,Ytrain_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.96      0.78      0.86        60\n",
      "          2       0.97      0.59      0.73       100\n",
      "          3       0.33      1.00      0.50        20\n",
      "          4       0.89      0.93      0.91        80\n",
      "          5       0.78      0.86      0.82        80\n",
      "          6       0.83      0.82      0.82        60\n",
      "\n",
      "avg / total       0.86      0.80      0.81       400\n",
      "\n",
      "[[47  0  3  1  0  9]\n",
      " [ 1 59 23  3 14  0]\n",
      " [ 0  0 20  0  0  0]\n",
      " [ 0  0  4 74  1  1]\n",
      " [ 1  2  6  2 69  0]\n",
      " [ 0  0  4  3  4 49]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(target_test, Ytest_predict))\n",
    "print(confusion_matrix(target_test, Ytest_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = np.array(target)\n",
    "np.savetxt('train_text_targets.csv', tar, header= 'classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = np.array(target_test)\n",
    "np.savetxt('test_text_targets.csv', tar, header= 'classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
