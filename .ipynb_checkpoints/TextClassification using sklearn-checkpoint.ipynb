{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#Traversing the path and enlisting the folders and files\n",
    "x = [os.path.join(r,file) for r,d,f in os.walk(r'''newsgroups''') for file in f]\n",
    "print(len(x))\n",
    "y = [os.path.join(r,direc) for r,d,f in os.walk(r'''newsgroups''') for direc in d]\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e234602239c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m19996\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtest_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m      \u001b[0mtrain_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#Spliting the files into Train and Test files\n",
    "test_file = []\n",
    "train_file = x\n",
    "print(len(train_file))\n",
    "for i in range(0,19996,50):\n",
    "    test_file.append(x[i])\n",
    "for file in test_file:\n",
    "     train_file.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_file))\n",
    "print(len(train_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a complete list of stop words which also includes some special characters present in the texts of data\n",
    "from stop_words import get_stop_words\n",
    "stop_words = get_stop_words('english')\n",
    "del stop_words[-1]\n",
    "stop_words_i = [word.title() for word in stop_words]\n",
    "stop_words_u = [word.upper() for word in stop_words]\n",
    "others = [str(i) for i in range(101)] + [str(i)+\".\" for i in range(101)]\n",
    "sp = [\"''\",'\"\"',\"'\",'\"','!','~','`','!','@','#','$','%','^','&','*','/','-','+','_',':)','**','***','_/','----------------------------------------------------------------------','[]','{}','()','\\\\','|','[',']','(','|>','>>','<<',\n",
    "')','{','}',',','.','?','=','<=','>=','<','>',':',';','A','B','Q','W','E','R','T','Y','U','I','O','P','S','D','F','G','H','J','K','L','Z','X','C','V','N','M','A.','B.','Q.','W.','E.','R.','T.','Y.','U.','I.','O.','P.','S.','D.','F.','G.','H.','J.','K.','L.','Z.','X.','C.','V.','N.','M.']\n",
    "stop_words = stop_words + stop_words_i + stop_words_u + others + sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the vocabulary \n",
    "vocab = {}\n",
    "for j in range(0,len(train_file)):\n",
    "    text = word_tokenize(open(train_file[j]).read()) #Converting string words into tokens\n",
    "    for k in text:\n",
    "        if k not in stop_words:\n",
    "            if vocab.get(k)!= None:\n",
    "                vocab[k]+=1\n",
    "            else:\n",
    "                vocab[k]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = vocab\n",
    "voc = sorted(voc.items(), key=lambda x: x[1], reverse=True) #Sorting the words according to their frequencies in the dataset\n",
    "col = voc[0:2000] #Choosing top 2000 most occuring words\n",
    "col = dict(col) \n",
    "columns = list(col.keys()) #list of all 2000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating 5 2-D arrays with every value 0\n",
    "# getting the memory error if done in one single matrix thats why created 5\n",
    "a1 = np.array([[0] * len(col) for _ in range(5000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2 = np.array([[0] * len(col) for _ in range(5000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a3 = np.array([[0] * len(col) for _ in range(5000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a4 = np.array([[0] * len(col) for _ in range(4000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traversing the text files once again \n",
    "# Travering each word and marking its frequency if it one of the most occuring word\n",
    "# Doing same for all 5 2D arrays\n",
    "# Converting the arrays into dataframes\n",
    "\n",
    "for j in range(0,5000):\n",
    "    text = word_tokenize(open(train_file[j]).read())\n",
    "    for k in text:\n",
    "        if k in columns:\n",
    "            pos = columns.index(k)\n",
    "            a1[j][pos] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1= pd.DataFrame(data= a1, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0,5000):\n",
    "    text = word_tokenize(open(train_file[j+5000]).read())\n",
    "    for k in text:\n",
    "        if k in columns:\n",
    "            pos = columns.index(k)\n",
    "            a2[j][pos] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2= pd.DataFrame(data= a2, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0,5000):\n",
    "    text = word_tokenize(open(train_file[j+10000]).read())\n",
    "    for k in text:\n",
    "        if k in columns:\n",
    "            pos = columns.index(k)\n",
    "            a3[j][pos] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3= pd.DataFrame(data= a3, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0,4000):\n",
    "    text = word_tokenize(open(train_file[j+15000]).read())\n",
    "    for k in text:\n",
    "        if k in columns:\n",
    "            pos = columns.index(k)\n",
    "            a4[j][pos] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d4= pd.DataFrame(data= a4, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a5 = np.array([[0] * len(col) for _ in range(597)])\n",
    "\n",
    "for j in range(len(a5)):\n",
    "    text = word_tokenize(open(train_file[j+19000]).read())\n",
    "    for k in text:\n",
    "        if k in columns:\n",
    "            pos = columns.index(k)\n",
    "            a5[j][pos] += 1\n",
    "\n",
    "d5= pd.DataFrame(data= a5, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending all 5 dataframes into 1\n",
    "df = d1.append([d2,d3,d4,d5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using dataframe as matrix once again\n",
    "data = df.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving dataframe into a CSV file\n",
    "df.to_csv(\"train_text.csv\", sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a names of the groups files belong to\n",
    "t = []\n",
    "for group in train_file:\n",
    "    g = group.split('\\\\')[1]\n",
    "    t.append(g)\n",
    "t[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Making a dictionary of groups to give them an integer value which shows class\n",
    "#  I have classified 20 groups into 6 classes viz, Religion, Computer, Vehicles, Sports, Politics and for_sale\n",
    "dic = {'alt.atheism':1,\n",
    " 'comp.graphics':2,\n",
    " 'comp.os.ms-windows.misc':2,\n",
    " 'comp.sys.ibm.pc.hardware':2,\n",
    " 'comp.sys.mac.hardware':2,\n",
    " 'comp.windows.x':2,\n",
    " 'misc.forsale':3,\n",
    " 'rec.autos':4,\n",
    " 'rec.motorcycles':4,\n",
    " 'rec.sport.baseball':4,\n",
    " 'rec.sport.hockey':4,\n",
    " 'sci.crypt':5,\n",
    " 'sci.electronics':5,\n",
    " 'sci.med':5,\n",
    " 'sci.space':5,\n",
    " 'soc.religion.christian':1,\n",
    " 'talk.politics.guns':6,\n",
    " 'talk.politics.mideast':6,\n",
    " 'talk.politics.misc':6,\n",
    " 'talk.religion.misc':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maping from the dictionary the integer value to each group\n",
    "target = []\n",
    "for group in train_file:\n",
    "    g = group.split('\\\\')[1]\n",
    "    target.append(dic[g])\n",
    "\n",
    "# Now we have 2 datasets one data which contains the frequencies of words\n",
    "# second is target datasets containg the classes of texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spliting the Train data \n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(data, target, random_state = 1, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(Xtrain,Ytrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(Xtest,Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doing same procedure for Test_file also \n",
    "a_test = np.array([[0] * len(col) for _ in range(len(test_file))])\n",
    "\n",
    "for j in range(len(a_test)):\n",
    "    text = word_tokenize(open(test_file[j]).read())\n",
    "    for k in text:\n",
    "        if k in columns:\n",
    "            pos = columns.index(k)\n",
    "            a_test[j][pos] += 1\n",
    "            \n",
    "d_test= pd.DataFrame(a_test, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test.to_csv(\"test_text.csv\", sep = ',')\n",
    "data_test = d_test.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_test = []\n",
    "for group in test_file:\n",
    "    g = group.split('\\\\')[1]\n",
    "    target_test.append(dic[g])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(data_test, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytrain_predict = clf.predict(Xtest)\n",
    "Ytest_predict = clf.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(Ytest,Ytrain_predict))\n",
    "print(confusion_matrix(Ytest,Ytrain_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(target_test, Ytest_predict))\n",
    "print(confusion_matrix(target_test, Ytest_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = np.array(target)\n",
    "np.savetxt('train_text_targets.csv', tar, header= 'classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = np.array(target_test)\n",
    "np.savetxt('test_text_targets.csv', tar, header= 'classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
